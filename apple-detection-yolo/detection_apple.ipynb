{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"detection_apple.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RjkGJftsLqKM","colab_type":"text"},"source":["# Começando\n","\n","Este notebook foi executado utilizando o serviço gratuito Google Colaboratory. As instruções fornecidas a seguir vão te ajudar a montar o ambiente necessário para que essa aplicação seja clonada/baixada e executada em sua máquina, como também uma explicação sucinta de algumas funcionalidades."]},{"cell_type":"markdown","metadata":{"id":"H94YtDux-u4_","colab_type":"text"},"source":["# Modificando o ambiente\n","\n","O Colab utiliza como default o ensorflow na versão 2.0. Para algumas bibliotecas, como por exemplo a que será utilizada nesta implementação, a versão 2.0 apresenta inúmeras incompatibilidades de versão, portanto para evitar problemas futuros, a versão utilizada do tensorflow será a 1.15.2. Pensando em problemas deste genêro, o colab desenvolveu o comando mágico: **%tensorflow_version 1.x**, em que modifica todo o ambiente de execução para atuar em uma versão estável do tensorflow 1.15.2. "]},{"cell_type":"code","metadata":{"id":"ox3MEhU3DnLj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593959168907,"user_tz":180,"elapsed":6045,"user":{"displayName":"Williana Luzia Sousa Leite UFC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhANZ4A6sqm0wIfvSKnX2l-Un1EaXAVgw3ID_4J=s64","userId":"05126895259861665891"}},"outputId":"c8a116d2-92ea-4d1b-a01d-6b6f609b4146"},"source":["%tensorflow_version 1.x\n","import tensorflow\n","print(tensorflow.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HtMJS5xwA0vy","colab_type":"text"},"source":["# Instalação\n","\n","Instalando a biblioteca que será utilizada para criação do modelo. "]},{"cell_type":"code","metadata":{"id":"6ci291LJD3DY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1593959178562,"user_tz":180,"elapsed":8182,"user":{"displayName":"Williana Luzia Sousa Leite UFC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhANZ4A6sqm0wIfvSKnX2l-Un1EaXAVgw3ID_4J=s64","userId":"05126895259861665891"}},"outputId":"32226507-83fd-4b0c-b125-0b970845f46a"},"source":["!pip install imageai"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting imageai\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/99/4023e191a343fb23f01ae02ac57a5ca58037c310e8d8c62f87638a3bafc7/imageai-2.1.5-py3-none-any.whl (180kB)\n","\r\u001b[K     |█▉                              | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from imageai) (2.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageai) (1.18.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imageai) (3.2.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageai) (7.0.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imageai) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->imageai) (1.12.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (1.2.0)\n","Installing collected packages: imageai\n","Successfully installed imageai-2.1.5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UQoDOf4aBSn3","colab_type":"text"},"source":["# Acessando os dados\n","\n","Existem algumas formas de acessar arquivos pessoais atravéz dos notebooks do colab, as duas principais são: importação direta utilizando a memória em disco disponibilizada pelo Colab, e a outra forma seria conecta o notebook a algum drive. Para este trabalho será utilizada a segunda abordagem, visto que minha conta drive fornece memória suficiente para armazenar todas as informações relevantes deste trabalho, a operação é apresentada na célula a seguir. Note que será utilizado uma conta drive diferente da conta em que este notebook está hospedado, pontanto sempre que se quiser referenciar algum arquivo/pasta o caminho absoluto deve ser utilizado. Caso o seu objetivo seja reproduzir este tutorial, faça o download da base de dados, através deste link: [apple_detection_dataset.zip](https://github.com/OlafenwaMoses/AppleDetection/releases/download/v1/apple_detection_dataset.zip), descompacte e faça o upload do dataset para alguma conta drive de sua preferência, fique atento para qual caminho você está salvando a pasta, para utilizar nos próximos passos. Vale ressaltar que o caminho raiz setado na função **mount**, é o que será utilizado para acessar o drive que possui o dataset, no meu caso, setei como caminho raiz '/content/drive', a partir deste ponto qualquer pasta do drive pode ser acessada exatamente igual como utilizada em um sistema de arquivos comum. "]},{"cell_type":"code","metadata":{"id":"yTLLQWnPEQ8y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1593959209226,"user_tz":180,"elapsed":29331,"user":{"displayName":"Williana Luzia Sousa Leite UFC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhANZ4A6sqm0wIfvSKnX2l-Un1EaXAVgw3ID_4J=s64","userId":"05126895259861665891"}},"outputId":"0f79f50d-edbe-4407-b740-ebfd37383f33"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#Clique no link a seguir para dar permissão ao colab para utilizar a conta drive, em seguida copie o código de autorização e cole no input requerido"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NBIwgXvDHWWY","colab_type":"text"},"source":["# Criação e configuração do modelo\n","\n","A classe que será utilizada para instanciação do modelo é a **DetectionModelTrainer**, que permite modificar o tipo de modelo para YOLOv3. Em seguida, o tipo do modelo deve ser informado, para isto será utilizado a função **setModelTypeAsYOLOv3**. Logo após, o diretório deve ser informado através da função **setDataDirectory** com o parâmetro obrigatório: **data_directory**, note que o caminho a ser passado neste parâmetro deve corresponder ao caminho referente a localização do dataset no drive que você fez upload, fique atento para informar o caminho raiz correto que você inseriu nos passos anteriores. Por fim, antes de iniciar o treinamento, algumas configurações devem ser inseridas através da função **setTrainConfig** que possui os seguintes parâmetros:\n","\n","\n","* **object_names_array (obrigatório):** Lista com o nome das classes, para este tutorial são duas: apple, damaged_apple.  \n","\n","* **batch_size (opcional):** Tamanho do batch, neste caso escolhi 8.\n","\n","* **num_experiments (obrigatório):** Número de épocas que o modelo irá treinar, neste caso escolhi 50.\n","\n","* **train_from_pretrained_model (opcional):** Caminho para um modelo YOLOv3 pré-treinado, neste caso utilizarei os pesos disponíveis neste link: [pretrained-yolov3.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5)\n","\n","Finalmente, após configurado o modelo pode ser treinado chamando a função: **trainModel**\n","\n"]},{"cell_type":"code","metadata":{"id":"nExIO-X0KHHP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593980333144,"user_tz":180,"elapsed":18128417,"user":{"displayName":"Williana Luzia Sousa Leite UFC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhANZ4A6sqm0wIfvSKnX2l-Un1EaXAVgw3ID_4J=s64","userId":"05126895259861665891"}},"outputId":"8c366f7c-3022-4151-c01c-b2809f653037"},"source":["from imageai.Detection.Custom import DetectionModelTrainer #importação da classe\n","\n","trainer = DetectionModelTrainer() #Instanciação do modelo\n","trainer.setModelTypeAsYOLOv3() #Setando o tipo de modelo para YOLOv3\n","trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset\") #Informando o diretório do dataset, observe que este é o meu caminho, seu deverá ser diferente.\n","trainer.setTrainConfig(object_names_array=[\"apple\", \"damaged_apple\"], batch_size=8, num_experiments=50, train_from_pretrained_model=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/pretrained-yolov3.h5\") #setando as congirações\n","trainer.trainModel() #Iniciando o treinamento"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Generating anchor boxes for training images and annotation...\n","Average IOU for 9 anchors: 0.78\n","Anchor Boxes generated.\n","Detection configuration saved in  /content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/json/detection_config.json\n","Training on: \t['apple', 'damaged_apple']\n","Training with Batch Size:  8\n","Number of Experiments:  50\n","Training with transfer learning from pretrained Model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n","  warnings.warn('`epsilon` argument is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/50\n","568/568 [==============================] - 461s 811ms/step - loss: 45.5876 - yolo_layer_7_loss: 4.9307 - yolo_layer_8_loss: 14.8798 - yolo_layer_9_loss: 25.7772 - val_loss: 18.9418 - val_yolo_layer_7_loss: 2.2429 - val_yolo_layer_8_loss: 7.1572 - val_yolo_layer_9_loss: 11.4811\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/50\n","568/568 [==============================] - 407s 717ms/step - loss: 21.1502 - yolo_layer_7_loss: 2.2163 - yolo_layer_8_loss: 7.6925 - yolo_layer_9_loss: 11.2414 - val_loss: 19.2717 - val_yolo_layer_7_loss: 2.8807 - val_yolo_layer_8_loss: 6.5329 - val_yolo_layer_9_loss: 10.6165\n","Epoch 3/50\n","568/568 [==============================] - 404s 712ms/step - loss: 17.3149 - yolo_layer_7_loss: 2.0314 - yolo_layer_8_loss: 5.8207 - yolo_layer_9_loss: 9.4629 - val_loss: 15.4696 - val_yolo_layer_7_loss: 1.6584 - val_yolo_layer_8_loss: 5.9760 - val_yolo_layer_9_loss: 10.3412\n","Epoch 4/50\n","568/568 [==============================] - 392s 690ms/step - loss: 15.6208 - yolo_layer_7_loss: 1.6290 - yolo_layer_8_loss: 4.9538 - yolo_layer_9_loss: 9.0380 - val_loss: 25.0349 - val_yolo_layer_7_loss: 1.5389 - val_yolo_layer_8_loss: 5.4204 - val_yolo_layer_9_loss: 10.2629\n","Epoch 5/50\n","568/568 [==============================] - 400s 704ms/step - loss: 14.9817 - yolo_layer_7_loss: 1.5720 - yolo_layer_8_loss: 4.9243 - yolo_layer_9_loss: 8.4853 - val_loss: 10.4193 - val_yolo_layer_7_loss: 1.5043 - val_yolo_layer_8_loss: 5.7071 - val_yolo_layer_9_loss: 9.6848\n","Epoch 6/50\n","568/568 [==============================] - 415s 730ms/step - loss: 14.5594 - yolo_layer_7_loss: 1.6019 - yolo_layer_8_loss: 4.9103 - yolo_layer_9_loss: 8.0472 - val_loss: 14.9125 - val_yolo_layer_7_loss: 1.5876 - val_yolo_layer_8_loss: 5.8392 - val_yolo_layer_9_loss: 9.8472\n","Epoch 7/50\n","568/568 [==============================] - 408s 718ms/step - loss: 13.9164 - yolo_layer_7_loss: 1.4500 - yolo_layer_8_loss: 4.6004 - yolo_layer_9_loss: 7.8659 - val_loss: 18.2490 - val_yolo_layer_7_loss: 1.1551 - val_yolo_layer_8_loss: 5.1686 - val_yolo_layer_9_loss: 9.8931\n","Epoch 8/50\n","568/568 [==============================] - 396s 698ms/step - loss: 13.3606 - yolo_layer_7_loss: 1.2906 - yolo_layer_8_loss: 4.4424 - yolo_layer_9_loss: 7.6275 - val_loss: 28.0275 - val_yolo_layer_7_loss: 1.4349 - val_yolo_layer_8_loss: 5.5779 - val_yolo_layer_9_loss: 10.0764\n","Epoch 9/50\n","568/568 [==============================] - 396s 697ms/step - loss: 12.9507 - yolo_layer_7_loss: 1.2516 - yolo_layer_8_loss: 4.3325 - yolo_layer_9_loss: 7.3666 - val_loss: 8.2657 - val_yolo_layer_7_loss: 1.1170 - val_yolo_layer_8_loss: 5.5137 - val_yolo_layer_9_loss: 10.0983\n","Epoch 10/50\n","568/568 [==============================] - 399s 703ms/step - loss: 12.6567 - yolo_layer_7_loss: 1.2340 - yolo_layer_8_loss: 4.3224 - yolo_layer_9_loss: 7.1003 - val_loss: 19.8719 - val_yolo_layer_7_loss: 1.5279 - val_yolo_layer_8_loss: 5.2234 - val_yolo_layer_9_loss: 9.6557\n","Epoch 11/50\n","568/568 [==============================] - 386s 679ms/step - loss: 12.0061 - yolo_layer_7_loss: 0.9915 - yolo_layer_8_loss: 3.8957 - yolo_layer_9_loss: 7.1189 - val_loss: 24.9356 - val_yolo_layer_7_loss: 1.3294 - val_yolo_layer_8_loss: 5.4709 - val_yolo_layer_9_loss: 9.7588\n","Epoch 12/50\n","568/568 [==============================] - 399s 702ms/step - loss: 12.3709 - yolo_layer_7_loss: 1.2622 - yolo_layer_8_loss: 4.0995 - yolo_layer_9_loss: 7.0092 - val_loss: 16.4300 - val_yolo_layer_7_loss: 1.9322 - val_yolo_layer_8_loss: 6.0141 - val_yolo_layer_9_loss: 9.4900\n","Epoch 13/50\n","568/568 [==============================] - 395s 696ms/step - loss: 11.7924 - yolo_layer_7_loss: 1.1052 - yolo_layer_8_loss: 3.9501 - yolo_layer_9_loss: 6.7371 - val_loss: 11.9486 - val_yolo_layer_7_loss: 1.2512 - val_yolo_layer_8_loss: 5.6377 - val_yolo_layer_9_loss: 9.7486\n","Epoch 14/50\n","568/568 [==============================] - 403s 710ms/step - loss: 11.8204 - yolo_layer_7_loss: 1.1892 - yolo_layer_8_loss: 3.9761 - yolo_layer_9_loss: 6.6552 - val_loss: 31.7497 - val_yolo_layer_7_loss: 1.5252 - val_yolo_layer_8_loss: 5.9172 - val_yolo_layer_9_loss: 10.3487\n","Epoch 15/50\n","568/568 [==============================] - 402s 707ms/step - loss: 11.4648 - yolo_layer_7_loss: 1.0569 - yolo_layer_8_loss: 3.7753 - yolo_layer_9_loss: 6.6326 - val_loss: 19.4064 - val_yolo_layer_7_loss: 1.3209 - val_yolo_layer_8_loss: 5.6674 - val_yolo_layer_9_loss: 9.7069\n","Epoch 16/50\n","568/568 [==============================] - 409s 720ms/step - loss: 11.3747 - yolo_layer_7_loss: 1.1184 - yolo_layer_8_loss: 3.7681 - yolo_layer_9_loss: 6.4882 - val_loss: 16.5209 - val_yolo_layer_7_loss: 1.3947 - val_yolo_layer_8_loss: 5.8482 - val_yolo_layer_9_loss: 10.3760\n","Epoch 17/50\n","568/568 [==============================] - 405s 714ms/step - loss: 11.0793 - yolo_layer_7_loss: 1.0648 - yolo_layer_8_loss: 3.5975 - yolo_layer_9_loss: 6.4170 - val_loss: 32.6124 - val_yolo_layer_7_loss: 1.2609 - val_yolo_layer_8_loss: 5.8021 - val_yolo_layer_9_loss: 10.4974\n","Epoch 18/50\n","568/568 [==============================] - 409s 720ms/step - loss: 10.9239 - yolo_layer_7_loss: 0.9092 - yolo_layer_8_loss: 3.6069 - yolo_layer_9_loss: 6.4078 - val_loss: 11.3479 - val_yolo_layer_7_loss: 1.3714 - val_yolo_layer_8_loss: 5.7546 - val_yolo_layer_9_loss: 10.0920\n","Epoch 19/50\n","568/568 [==============================] - 416s 732ms/step - loss: 10.8670 - yolo_layer_7_loss: 1.0032 - yolo_layer_8_loss: 3.7326 - yolo_layer_9_loss: 6.1312 - val_loss: 17.4876 - val_yolo_layer_7_loss: 1.3659 - val_yolo_layer_8_loss: 5.9747 - val_yolo_layer_9_loss: 9.5128\n","Epoch 20/50\n","568/568 [==============================] - 413s 727ms/step - loss: 10.4903 - yolo_layer_7_loss: 0.8586 - yolo_layer_8_loss: 3.4715 - yolo_layer_9_loss: 6.1603 - val_loss: 13.7586 - val_yolo_layer_7_loss: 1.4045 - val_yolo_layer_8_loss: 6.0800 - val_yolo_layer_9_loss: 10.5343\n","Epoch 21/50\n","568/568 [==============================] - 411s 723ms/step - loss: 10.2494 - yolo_layer_7_loss: 0.9372 - yolo_layer_8_loss: 3.3014 - yolo_layer_9_loss: 6.0108 - val_loss: 16.9260 - val_yolo_layer_7_loss: 1.2794 - val_yolo_layer_8_loss: 6.0141 - val_yolo_layer_9_loss: 9.6839\n","Epoch 22/50\n","568/568 [==============================] - 407s 716ms/step - loss: 10.1449 - yolo_layer_7_loss: 0.8417 - yolo_layer_8_loss: 3.3606 - yolo_layer_9_loss: 5.9426 - val_loss: 24.2924 - val_yolo_layer_7_loss: 1.7931 - val_yolo_layer_8_loss: 6.2637 - val_yolo_layer_9_loss: 10.4009\n","Epoch 23/50\n","568/568 [==============================] - 409s 719ms/step - loss: 10.2442 - yolo_layer_7_loss: 0.8471 - yolo_layer_8_loss: 3.3780 - yolo_layer_9_loss: 6.0192 - val_loss: 14.3104 - val_yolo_layer_7_loss: 1.5833 - val_yolo_layer_8_loss: 6.0536 - val_yolo_layer_9_loss: 10.7735\n","Epoch 24/50\n","568/568 [==============================] - 414s 729ms/step - loss: 10.1372 - yolo_layer_7_loss: 0.9613 - yolo_layer_8_loss: 3.3618 - yolo_layer_9_loss: 5.8141 - val_loss: 21.7672 - val_yolo_layer_7_loss: 1.2495 - val_yolo_layer_8_loss: 5.7422 - val_yolo_layer_9_loss: 9.8017\n","Epoch 25/50\n","568/568 [==============================] - 407s 717ms/step - loss: 9.1212 - yolo_layer_7_loss: 0.7082 - yolo_layer_8_loss: 2.9890 - yolo_layer_9_loss: 5.4240 - val_loss: 16.2899 - val_yolo_layer_7_loss: 1.0158 - val_yolo_layer_8_loss: 5.8498 - val_yolo_layer_9_loss: 10.3724\n","Epoch 26/50\n","568/568 [==============================] - 412s 725ms/step - loss: 8.7438 - yolo_layer_7_loss: 0.7205 - yolo_layer_8_loss: 2.8896 - yolo_layer_9_loss: 5.1337 - val_loss: 17.5728 - val_yolo_layer_7_loss: 1.3212 - val_yolo_layer_8_loss: 6.5215 - val_yolo_layer_9_loss: 10.5033\n","Epoch 27/50\n","568/568 [==============================] - 402s 708ms/step - loss: 8.4147 - yolo_layer_7_loss: 0.6750 - yolo_layer_8_loss: 2.8606 - yolo_layer_9_loss: 4.8791 - val_loss: 13.2532 - val_yolo_layer_7_loss: 0.9876 - val_yolo_layer_8_loss: 5.6280 - val_yolo_layer_9_loss: 10.3551\n","Epoch 28/50\n","568/568 [==============================] - 394s 694ms/step - loss: 8.3338 - yolo_layer_7_loss: 0.6153 - yolo_layer_8_loss: 2.7198 - yolo_layer_9_loss: 4.9987 - val_loss: 9.0383 - val_yolo_layer_7_loss: 1.0136 - val_yolo_layer_8_loss: 5.8211 - val_yolo_layer_9_loss: 10.3367\n","Epoch 29/50\n","568/568 [==============================] - 398s 701ms/step - loss: 8.4990 - yolo_layer_7_loss: 0.6949 - yolo_layer_8_loss: 2.7813 - yolo_layer_9_loss: 5.0227 - val_loss: 15.7943 - val_yolo_layer_7_loss: 0.9582 - val_yolo_layer_8_loss: 5.7792 - val_yolo_layer_9_loss: 10.0317\n","Epoch 30/50\n","568/568 [==============================] - 394s 694ms/step - loss: 8.3192 - yolo_layer_7_loss: 0.6237 - yolo_layer_8_loss: 2.7455 - yolo_layer_9_loss: 4.9500 - val_loss: 5.9086 - val_yolo_layer_7_loss: 1.1399 - val_yolo_layer_8_loss: 6.2619 - val_yolo_layer_9_loss: 10.6301\n","Epoch 31/50\n","568/568 [==============================] - 400s 705ms/step - loss: 8.1355 - yolo_layer_7_loss: 0.6096 - yolo_layer_8_loss: 2.7549 - yolo_layer_9_loss: 4.7710 - val_loss: 11.8145 - val_yolo_layer_7_loss: 1.1248 - val_yolo_layer_8_loss: 6.2512 - val_yolo_layer_9_loss: 10.4552\n","Epoch 32/50\n","568/568 [==============================] - 411s 724ms/step - loss: 8.1485 - yolo_layer_7_loss: 0.6838 - yolo_layer_8_loss: 2.7807 - yolo_layer_9_loss: 4.6839 - val_loss: 18.9805 - val_yolo_layer_7_loss: 1.0848 - val_yolo_layer_8_loss: 6.0121 - val_yolo_layer_9_loss: 10.7232\n","Epoch 33/50\n","568/568 [==============================] - 397s 699ms/step - loss: 8.2158 - yolo_layer_7_loss: 0.6749 - yolo_layer_8_loss: 2.6973 - yolo_layer_9_loss: 4.8436 - val_loss: 19.1172 - val_yolo_layer_7_loss: 1.1301 - val_yolo_layer_8_loss: 6.2279 - val_yolo_layer_9_loss: 10.5172\n","Epoch 34/50\n","568/568 [==============================] - 397s 700ms/step - loss: 8.0115 - yolo_layer_7_loss: 0.6585 - yolo_layer_8_loss: 2.6967 - yolo_layer_9_loss: 4.6562 - val_loss: 26.0920 - val_yolo_layer_7_loss: 1.1080 - val_yolo_layer_8_loss: 6.5894 - val_yolo_layer_9_loss: 10.2126\n","Epoch 35/50\n","568/568 [==============================] - 399s 702ms/step - loss: 7.8336 - yolo_layer_7_loss: 0.5461 - yolo_layer_8_loss: 2.6370 - yolo_layer_9_loss: 4.6505 - val_loss: 12.9441 - val_yolo_layer_7_loss: 1.0373 - val_yolo_layer_8_loss: 5.8032 - val_yolo_layer_9_loss: 10.4043\n","Epoch 36/50\n","568/568 [==============================] - 407s 717ms/step - loss: 8.1145 - yolo_layer_7_loss: 0.6340 - yolo_layer_8_loss: 2.7163 - yolo_layer_9_loss: 4.7643 - val_loss: 25.6499 - val_yolo_layer_7_loss: 1.1562 - val_yolo_layer_8_loss: 5.8418 - val_yolo_layer_9_loss: 10.7631\n","Epoch 37/50\n","568/568 [==============================] - 395s 695ms/step - loss: 7.9038 - yolo_layer_7_loss: 0.6168 - yolo_layer_8_loss: 2.5723 - yolo_layer_9_loss: 4.7148 - val_loss: 15.5514 - val_yolo_layer_7_loss: 1.0080 - val_yolo_layer_8_loss: 6.1695 - val_yolo_layer_9_loss: 10.3796\n","Epoch 38/50\n","568/568 [==============================] - 397s 700ms/step - loss: 7.9544 - yolo_layer_7_loss: 0.6581 - yolo_layer_8_loss: 2.6598 - yolo_layer_9_loss: 4.6365 - val_loss: 26.1703 - val_yolo_layer_7_loss: 1.1304 - val_yolo_layer_8_loss: 6.0549 - val_yolo_layer_9_loss: 10.6861\n","Epoch 39/50\n","568/568 [==============================] - 394s 694ms/step - loss: 7.8202 - yolo_layer_7_loss: 0.6131 - yolo_layer_8_loss: 2.6150 - yolo_layer_9_loss: 4.5920 - val_loss: 24.1752 - val_yolo_layer_7_loss: 1.3094 - val_yolo_layer_8_loss: 6.5034 - val_yolo_layer_9_loss: 10.3938\n","Epoch 40/50\n","568/568 [==============================] - 394s 694ms/step - loss: 7.9133 - yolo_layer_7_loss: 0.6336 - yolo_layer_8_loss: 2.6589 - yolo_layer_9_loss: 4.6207 - val_loss: 21.6797 - val_yolo_layer_7_loss: 0.7144 - val_yolo_layer_8_loss: 5.2704 - val_yolo_layer_9_loss: 10.8581\n","Epoch 41/50\n","568/568 [==============================] - 397s 699ms/step - loss: 7.9045 - yolo_layer_7_loss: 0.6443 - yolo_layer_8_loss: 2.6758 - yolo_layer_9_loss: 4.5844 - val_loss: 24.9719 - val_yolo_layer_7_loss: 1.3588 - val_yolo_layer_8_loss: 6.6950 - val_yolo_layer_9_loss: 10.3585\n","Epoch 42/50\n","568/568 [==============================] - 394s 693ms/step - loss: 7.8635 - yolo_layer_7_loss: 0.5532 - yolo_layer_8_loss: 2.6244 - yolo_layer_9_loss: 4.6859 - val_loss: 18.9963 - val_yolo_layer_7_loss: 1.0311 - val_yolo_layer_8_loss: 5.7576 - val_yolo_layer_9_loss: 10.4417\n","Epoch 43/50\n","568/568 [==============================] - 389s 685ms/step - loss: 7.7656 - yolo_layer_7_loss: 0.5634 - yolo_layer_8_loss: 2.5248 - yolo_layer_9_loss: 4.6775 - val_loss: 24.1727 - val_yolo_layer_7_loss: 1.1679 - val_yolo_layer_8_loss: 5.8952 - val_yolo_layer_9_loss: 10.3525\n","Epoch 44/50\n","568/568 [==============================] - 392s 690ms/step - loss: 7.8571 - yolo_layer_7_loss: 0.5827 - yolo_layer_8_loss: 2.5417 - yolo_layer_9_loss: 4.7327 - val_loss: 14.4182 - val_yolo_layer_7_loss: 1.1503 - val_yolo_layer_8_loss: 6.0246 - val_yolo_layer_9_loss: 10.4020\n","Epoch 45/50\n","568/568 [==============================] - 392s 690ms/step - loss: 7.9176 - yolo_layer_7_loss: 0.5705 - yolo_layer_8_loss: 2.6295 - yolo_layer_9_loss: 4.7176 - val_loss: 10.7639 - val_yolo_layer_7_loss: 1.1431 - val_yolo_layer_8_loss: 6.0389 - val_yolo_layer_9_loss: 10.5709\n","Epoch 46/50\n","568/568 [==============================] - 389s 686ms/step - loss: 7.7069 - yolo_layer_7_loss: 0.5468 - yolo_layer_8_loss: 2.4600 - yolo_layer_9_loss: 4.7002 - val_loss: 24.7991 - val_yolo_layer_7_loss: 1.0809 - val_yolo_layer_8_loss: 6.0378 - val_yolo_layer_9_loss: 10.4650\n","Epoch 47/50\n","568/568 [==============================] - 386s 680ms/step - loss: 7.5500 - yolo_layer_7_loss: 0.5020 - yolo_layer_8_loss: 2.4359 - yolo_layer_9_loss: 4.6121 - val_loss: 22.8845 - val_yolo_layer_7_loss: 1.2456 - val_yolo_layer_8_loss: 6.2919 - val_yolo_layer_9_loss: 10.4662\n","Epoch 48/50\n","568/568 [==============================] - 393s 692ms/step - loss: 7.8574 - yolo_layer_7_loss: 0.5578 - yolo_layer_8_loss: 2.6318 - yolo_layer_9_loss: 4.6678 - val_loss: 23.6773 - val_yolo_layer_7_loss: 1.2108 - val_yolo_layer_8_loss: 5.9391 - val_yolo_layer_9_loss: 10.6249\n","Epoch 49/50\n","568/568 [==============================] - 394s 695ms/step - loss: 7.9969 - yolo_layer_7_loss: 0.6087 - yolo_layer_8_loss: 2.6773 - yolo_layer_9_loss: 4.7108 - val_loss: 10.2420 - val_yolo_layer_7_loss: 0.7579 - val_yolo_layer_8_loss: 5.6948 - val_yolo_layer_9_loss: 10.4830\n","Epoch 50/50\n","568/568 [==============================] - 398s 700ms/step - loss: 7.8055 - yolo_layer_7_loss: 0.6377 - yolo_layer_8_loss: 2.6022 - yolo_layer_9_loss: 4.5656 - val_loss: 17.6192 - val_yolo_layer_7_loss: 1.1509 - val_yolo_layer_8_loss: 6.3172 - val_yolo_layer_9_loss: 10.4362\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LKfUu-QtpeUO","colab_type":"text"},"source":["# Carregando o modelo\n","\n","Depois que o treinamento começa, é criado uma pasta chamada json que contém um arquivo de configuração .json referente ao modelo e também é criado uma pasta chamada models que guarda os pesos de cada época. Após o treinamento para carregar o modelo de volta utilize a classe **CustomObjectDetection**, crie uma instância, set o modelo de acordo com o que foi utilizado, no nosso caso: YOLOv3. Em seguida informe o diretório que o modelo (arquivo .h5) estar, por meio da função **setModelPath** passando como argumento do parâmetro **detection_model_path**, tenha em mente que como o modelo salvou os pesos de todas as épocas, logo você tem a liberdade de escolher qualquer época para analisar,  utilizando o critério que preferir, para este trabalho analisei apenas o **loss**, portanto escolhi a época 47, com **loss = 7.5500**, visto que foi a época que obteve menor **loss**. Por fim, deve ser informado qual o diretório que o arquivo de configuração se encontra, usando a função **setJsonPath**, por meio do parâmetro **configuration_json**. Agora a função **loadModel** pode ser chamada para carregar o modelo."]},{"cell_type":"code","metadata":{"id":"A_IhYSoGDucP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593980569177,"user_tz":180,"elapsed":19953,"user":{"displayName":"Williana Luzia Sousa Leite UFC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhANZ4A6sqm0wIfvSKnX2l-Un1EaXAVgw3ID_4J=s64","userId":"05126895259861665891"}}},"source":["from imageai.Detection.Custom import CustomObjectDetection\n","import os\n","\n","detector = CustomObjectDetection() #Criando a instância do modelo\n","detector.setModelTypeAsYOLOv3() #setando o tipo do modelo\n","detector.setModelPath(detection_model_path=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/models/detection_model-ex-047--loss-0007.550.h5\") #informando o diretório que se encontra o modelo, lembre de alterar este caminho para o correspondente em seu drive \n","detector.setJsonPath(configuration_json=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/json/detection_config.json\") #informando o diretório que se encontra o arquivo de configuração, lembre de alterar este caminho para o correspondente em seu drive \n","detector.loadModel()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04QjYaf8Q0ut","colab_type":"text"},"source":["# Analisando a saída do detector\n","\n","A classe **CustomObjectDetection** fornece um método bastante simples e eficiente para fazer as detecções em uma determinada imagem, que se chama: **detectObjectsFromImage**, que possui os seguintes parâmetros:\n","\n","- **input_image:** deve-se informar o caminho da imagem que se quer detectar os objetos\n","- **minimum_percentage_probability:** que determina a integridade dos resultados da detecção. A redução do valor mostra mais objetos, enquanto o aumento do valor garante a detecção de objetos com a maior precisão. Acredito que funciona como Intersection over Union(IoU).\n","- **output_image_path:** deve-se informar o caminho que a imagem detectada deve ser salva.\n","\n","O retorno desta função é um array de dicionários, onde cada objeto tem sua saída representada por um dict contendo as seguintes informações:\n","\n","- Nome da classe detectada\n","- O percentual de probabilidade, acredito que seja uma espécie de nível de confiança\n","- E as coordenadas do objeto detectado"]},{"cell_type":"code","metadata":{"id":"biBOH0dUK_mQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593980579382,"user_tz":180,"elapsed":7456,"user":{"displayName":"Williana Luzia Sousa Leite UFC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhANZ4A6sqm0wIfvSKnX2l-Un1EaXAVgw3ID_4J=s64","userId":"05126895259861665891"}},"outputId":"e75f6bf0-927e-44ce-f747-1f0c4b6f6359"},"source":["detections = detector.detectObjectsFromImage(input_image=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/image.jpg\", minimum_percentage_probability=60, output_image_path=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/predict-image.jpg\")\n","\n","for detection in detections:\n","    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["damaged_apple  :  91.38688445091248  :  [163, 105, 443, 324]\n","apple  :  98.62603545188904  :  [12, 0, 192, 61]\n","apple  :  99.37939643859863  :  [200, 1, 363, 69]\n","apple  :  99.67042803764343  :  [371, 0, 568, 60]\n","apple  :  92.88690090179443  :  [21, 13, 254, 173]\n","apple  :  97.8295087814331  :  [275, 17, 476, 173]\n","apple  :  96.79529666900635  :  [1, 86, 160, 311]\n","apple  :  94.382643699646  :  [453, 107, 624, 318]\n","apple  :  97.9244589805603  :  [493, 13, 628, 132]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Oc8Dk2XjeChY","colab_type":"text"},"source":["# Avaliando o modelo\n","\n","Para avaliar o modelo será utilizado a métrica mean Average Precision(mAP). O mAP é a média de average precisions. O average precision deve ser calculado para cada classe, e o mesmo é calculado da seguinte forma:\n","\n","\n","![](https://miro.medium.com/max/1400/1*q6S0m6R6mQA1J6K30HZkvw.jpeg)\n","\n","Na imagem abaixo é apresentado um exemplo de como calcular o average precision, onde basicamente é o cálculo da área sob a curva em uma projeção de precisão e recall para cada classe para cada imagem.\n","\n","![](https://miro.medium.com/max/1400/1*TAuQ3UOA8xh_5wI5hwLHcg.jpeg)\n","\n","Para maiores infomações de como o mAP funciona, consulte este [link](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173).\n","\n","Para avaliar o modelo, será utilizado a classe **DetectionModelTrainer**. Como de constume antes deve ser informado o tipo do modelo, o diretório do dataset e por fim pode-se chamar a função **evaluateModel** que vai avaliar o modelo com a métrica mAP, que possui os seguintes parâmetros: \n","- **model_path:** caminho para o modelo\n","- **json_path:** caminho para o arquivo de configuração\n","- **iou_threshold:** qual o limiar para considerar uma predição como true positive\n","- **object_threshold:** é usado para definir a pontuação mínima de confiança para a avaliação do mAP \n","- **nms_threshold:** é usado para definir o valor mínimo de supressão não máxima para a avaliação do mAP. Este valor está relacionado a quantidade de detecções sobrepostas que serão aceitas"]},{"cell_type":"code","metadata":{"id":"-Z3R3P9TV_P3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1593980652915,"user_tz":180,"elapsed":64944,"user":{"displayName":"Williana Luzia Sousa Leite UFC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhANZ4A6sqm0wIfvSKnX2l-Un1EaXAVgw3ID_4J=s64","userId":"05126895259861665891"}},"outputId":"74a19648-582f-435c-b4f7-688ad1fcc7ee"},"source":["from imageai.Detection.Custom import DetectionModelTrainer\n","\n","trainer = DetectionModelTrainer()\n","trainer.setModelTypeAsYOLOv3()\n","trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/\")\n","metrics = trainer.evaluateModel(model_path=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/models/detection_model-ex-029--loss-0008.706.h5\", json_path=\"/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/json/detection_config.json\", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.4)\n","print(metrics)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Starting Model evaluation....\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n"],"name":"stderr"},{"output_type":"stream","text":["Model File:  /content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/models/detection_model-ex-029--loss-0008.706.h5 \n","\n","Using IoU :  0.5\n","Using Object Threshold :  0.3\n","Using Non-Maximum Suppression :  0.4\n","apple: 0.8272\n","damaged_apple: 0.6550\n","mAP: 0.7411\n","===============================\n","[{'model_file': '/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/models/detection_model-ex-029--loss-0008.706.h5', 'using_iou': 0.5, 'using_object_threshold': 0.3, 'using_non_maximum_suppression': 0.4, 'average_precision': {'apple': 0.8272384006477047, 'damaged_apple': 0.6550435058758989}, 'map': 0.7411409532618018}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mSgYjnQTewJ7","colab_type":"text"},"source":["# Detectando as imagens de teste\n","\n","Por fim, para uma melhor análise dos resultados obtidos, a célula a seguir percorre todas as imagens de um determinado diretório e as manda para detecção, exatamente como foi explicado anteriormente, só que agora aplicado a um conjunto de imagens."]},{"cell_type":"code","metadata":{"id":"MDT7EJsjWD1l","colab_type":"code","colab":{}},"source":["from os import listdir\n","\n","dir_images_teste = '/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/apple_dataset/validation/images/'\n","dir_images_teste_result = '/content/drive/My Drive/2020.1/VisaoComputacional/trabalho-final-apples-yolo/images_predict/'\n","dir_images = listdir(dir_images_teste)\n","for j in dir_images:\n","  detections = detector.detectObjectsFromImage(input_image=dir_images_teste + j, minimum_percentage_probability=60, output_image_path= dir_images_teste_result + j)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfT4QLUrFFfA","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}